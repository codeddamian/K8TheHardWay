##### Why we need KUBECONFIG
- Kubeconfig is a file that stores information about the clusters, users namespaces and authentication mechanisms. -it contains the 
config data needed to connect to and interact with one or more k8 clusters. More info on the k8 info
     - kube config file will tell us the location of the cluster you want to connect to what,
     - what user do you want to authenticate as
     - and what data is needed in order to authenticate -such as tokens or client certs
 
##### Generating the Kubeconfig files for the Cluster
- Kubeconfigs are generated using kubectl
   - Setup a config using kubectl config set-cluster, set-credentials, set-context [default context] and use-context default [to set the 
current context to the configuration  we provided]

- The kubeconfig file we will be generating for various components of the k8 cluster are
  - kubelet (one for each worker node)
  - kube-proxy
  - kube-controller-manager
  - kube-scheduler
  - Admin
 
- BAck to ~/ kthw
- Use the certificate to generate the kubeconfig files
- Also we need to set an ENVIRONMENT VARIABLE using the load balance private ip [KUBERNETES_ADDRESS=<PrivateIP.Loadnbalancer>
```
# Create an environment variable to store the address of the Kubernetes API, and set it to the private IP of your load balancer cloud server:

KUBERNETES_ADDRESS=<load balancer private ip>

# Generate a kubelet kubeconfig for each worker node:

for instance in <worker 1 hostname> <worker 2 hostname>; do
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://${KUBERNETES_ADDRESS}:6443 \      #To reach the kubeAPI it will use this variable to locate it and since we have an
                                    nginx that is loadbalacning both kubeAPI server we set the private ip of nginx 
    --kubeconfig=${instance}.kubeconfig
  kubectl config set-credentials system:node:${instance} \
    --client-certificate=${instance}.pem \
    --client-key=${instance}-key.pem \
    --embed-certs=true \
    --kubeconfig=${instance}.kubeconfig
  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:node:${instance} \
    --kubeconfig=${instance}.kubeconfig

  kubectl config use-context default --kubeconfig=${instance}.kubeconfig
done
# once executed a file named <hostname>.kubeconfig will now be in the ~/kthw directory 
```
```
# Generate a kube-proxy kubeconfig:

{
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://${KUBERNETES_ADDRESS}:6443 \
    --kubeconfig=kube-proxy.kubeconfig
  kubectl config set-credentials system:kube-proxy \
    --client-certificate=kube-proxy.pem \
    --client-key=kube-proxy-key.pem \
    --embed-certs=true \
    --kubeconfig=kube-proxy.kubeconfig
  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:kube-proxy \
    --kubeconfig=kube-proxy.kubeconfig

  kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig
}
```
```
# Generate a kube-controller-manager kubeconfig:

{
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \   #Kubecontroller manager doesn't need to go through the load balancer since it's on the 
                                              same controller node 
    --kubeconfig=kube-controller-manager.kubeconfig

  kubectl config set-credentials system:kube-controller-manager \
    --client-certificate=kube-controller-manager.pem \
    --client-key=kube-controller-manager-key.pem \
    --embed-certs=true \
    --kubeconfig=kube-controller-manager.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:kube-controller-manager \
    --kubeconfig=kube-controller-manager.kubeconfig

  kubectl config use-context default --kubeconfig=kube-controller-manager.kubeconfig
}
```
```
# Generate a kube-scheduler kubeconfig:

{
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=kube-scheduler.kubeconfig

  kubectl config set-credentials system:kube-scheduler \
    --client-certificate=kube-scheduler.pem \
    --client-key=kube-scheduler-key.pem \
    --embed-certs=true \
    --kubeconfig=kube-scheduler.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:kube-scheduler \
    --kubeconfig=kube-scheduler.kubeconfig

  kubectl config use-context default --kubeconfig=kube-scheduler.kubeconfig
}
```
```
# Generate an admin kubeconfig:

{
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.pem \
    --embed-certs=true \
    --server=https://127.0.0.1:6443 \
    --kubeconfig=admin.kubeconfig

  kubectl config set-credentials admin \
    --client-certificate=admin.pem \
    --client-key=admin-key.pem \
    --embed-certs=true \
    --kubeconfig=admin.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=admin \
    --kubeconfig=admin.kubeconfig

  kubectl config use-context default --kubeconfig=admin.kubeconfig
}
```

```
# Distributing the kube config file to the specific file location

# Now that we have generated the kubeconfig files that we will need in order to configure our Kubernetes cluster, we need to make sure
 that each cloud server has a copy of the kubeconfig files that it will need. In this lesson, we will distribute the kubeconfig files
 to each of the worker and controller nodes so that they will be in place for future lessons. After completing this lesson, each of
 your worker and controller nodes should have a copy of the kubeconfig files it needs.

# Here are the commands used in the demo. Be sure to replace the placeholders with the actual values from your cloud servers.

# Move kubeconfig files to the worker nodes:

scp <worker 1 hostname>.kubeconfig kube-proxy.kubeconfig cloud_user@<worker 1 public IP>:~/

scp <worker 2 hostname>.kubeconfig kube-proxy.kubeconfig cloud_user@<worker 2 public IP>:~/

# Move kubeconfig files to the controller nodes:

scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig cloud_user@<controller 1 public IP>:~/

scp admin.kubeconfig kube-controller-manager.kubeconfig kube-scheduler.kubeconfig cloud_user@<controller 2 public IP>:~/

```
