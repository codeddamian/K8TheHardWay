#### Cluster Networking
Networking is a central part of Kubernetes, but it can be challenging to understand exactly how it is expected to work.

There are 4 distinct networking problems to address:
- Highly-coupled container-to-container communications: this is solved by Pods and localhost communications.
- Pod-to-Pod communications: this is the primary focus of this document.
- Pod-to-Service communications: this is covered by Services.
- External-to-Service communications: this is also covered by Services.

Kubernetes is all about sharing machines between applications. Typically, sharing machines requires ensuring that two applications
do not try to use the same ports. Coordinating ports across multiple developers is very difficult to do at scale and exposes users
to cluster-level issues outside of their control.

Dynamic port allocation brings a lot of complications to the system - every application has to take ports as flags, the API servers
have to know how to insert dynamic port numbers into configuration blocks, services have to know how to find each other, etc.
Rather than deal with this, Kubernetes takes a different approach.

To learn about the Kubernetes networking model, see here.

How to implement the Kubernetes network model
The network model is implemented by the container runtime on each node. The most common container runtimes use Container Network
Interface (CNI) plugins to manage their network and security capabilities. Many different CNI plugins exist from many different
vendors. Some of these provide only basic features of adding and removing network interfaces, while others provide more
sophisticated solutions, such as integration with other container orchestration systems, running multiple CNI plugins, advanced
IPAM features etc.

K8 Networking model
- one virtual network for the whole cluster
- each pod has a unique IP on the cluster
- each service has a unique IP that is in a different range than pods IP

### Cluster Networking Architecture
- Cluster CIDR
- Service Cluster IP range - Not overlap CLuster cidr range 
- Pod CIDR -  Not overlap pod cidr on any of the worker node  
-

### Install Weave Net
We are now ready to set up networking in our Kubernetes cluster. This lesson guides you through the process of installing Weave
Net in the cluster. It also shows you how to test your cluster network to make sure that everything is working as expected so far.
After completing this lesson, you should have a functioning cluster network within your Kubernetes cluster.

- You can configure Weave Net like this:

 ##### First, log in to both worker nodes and enable IP forwarding:

sudo sysctl net.ipv4.conf.all.forwarding=1   #temporary

echo "net.ipv4.conf.all.forwarding=1" | sudo tee -a /etc/sysctl.conf  #permanent

The remaining commands can be done using kubectl. To connect with kubectl, you can either log in to one of the control nodes and
run kubectl there or open an SSH tunnel for port 6443 to the load balancer server and use kubectl locally.

#####  You can open the SSH tunnel by running this in a separate terminal. Leave the session open while you are working to keep
#####   the tunnel active:

ssh -L 6443:localhost:6443 cloud_user@<your Load balancer cloud server public IP>
kubectl get nodes
#### Install Weave Net like this:

kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')&env.IPALLOC_RANGE=10.200.0.0/16"
  -  10.200.0.0/16 -Cluster cidr

##### Now Weave Net is installed, but we need to test our network to make sure everything is working.

First, make sure the Weave Net pods are up and running:

kubectl get pods -n kube-system

#####  This should return two Weave Net pods, and look something like this:

NAME READY STATUS RESTARTS AGE
weave-net-m69xq 2/2 Running 0 11s
weave-net-vmb2n 2/2 Running 0 11s

# Next, we want to test that pods can connect to each other and that they can connect to services.
We will set up two Nginx pods and a service for those two pods. Then, we will create a busybox pod and use it to test 
connectivity to both Nginx pods and the service.

```
# First, create an Nginx deployment with 2 replicas:

cat << EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      run: nginx
  replicas: 2
  template:
    metadata:
      labels:
        run: nginx
    spec:
      containers:
      - name: my-nginx
        image: nginx
        ports:
        - containerPort: 80
EOF
```
```
# Next, create a service for that deployment so that we can test connectivity to services as well:

kubectl expose deployment/nginx
```
```
# Now let's start up another pod. We will use this pod to test our networking. We will test whether we can connect to the other pods and services from this pod.

kubectl run busybox --image=radial/busyboxplus:curl --command -- sleep 3600

POD_NAME=$(kubectl get pods -l run=busybox -o jsonpath="{.items[0].metadata.name}")
```
```
# Now let's get the IP addresses of our two Nginx pods:

kubectl get ep nginx   #ep=ENDPOINT

# There should be two IP addresses listed under ENDPOINTS, for example:

NAME ENDPOINTS AGE
nginx 10.200.0.2:80,10.200.128.1:80 50m

# Now let's make sure the busybox pod can connect to the Nginx pods on both of those IP addresses.

kubectl exec $POD_NAME -- curl <first nginx pod IP address>
kubectl exec $POD_NAME -- curl <second nginx pod IP address>

# Both commands should return some HTML with the title "Welcome to Nginx!" This means that we can successfully connect to other pods.

# Now let's verify that we can connect to services.

kubectl get svc

# This should display the IP address for our Nginx service. For example, in this case, the IP is 10.32.0.54:

NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
kubernetes ClusterIP 10.32.0.1 <none> 443/TCP 1h
nginx ClusterIP 10.32.0.54 <none> 80/TCP 53m

# Let's see if we can access the service from the busybox pod!

kubectl exec $POD_NAME -- curl <nginx service IP address>

# This should also return HTML with the title "Welcome to Nginx!"
```
- This means that we have successfully reached the Nginx service from inside a pod and that our networking configuration is working!

  Cleanup - Kubectl get deploy.
  kubectl delete deployment busy-box
  kubectl delete deployment nginx  #deleting deployment deletes pods 
  kubectl delete svc
  
